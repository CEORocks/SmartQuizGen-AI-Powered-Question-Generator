{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration - SQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "from nltk import tokenize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBold(string):\n",
    "    display(Markdown('**' + string + '**'))\n",
    "    \n",
    "#def printColor():\n",
    "#     display(Markdown('<span style=\"color:blue\">blue</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aren't really doing the answering of the questions, as is the true intention for the dataset, we'll merge the train and dev datasets into one. The test dataset is probably hidden, since there's a competition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/squad-v1/train-v1.1.json', orient='column')\n",
    "dev = pd.read_json('../data/squad-v1/dev-v1.1.json', orient='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, dev], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'University_of_Notre_Dame', 'paragra...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Beyoncé', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Montana', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Genocide', 'paragraphs': [{'context...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Antibiotics', 'paragraphs': [{'cont...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  version\n",
       "0  {'title': 'University_of_Notre_Dame', 'paragra...      1.1\n",
       "1  {'title': 'Beyoncé', 'paragraphs': [{'context'...      1.1\n",
       "2  {'title': 'Montana', 'paragraphs': [{'context'...      1.1\n",
       "3  {'title': 'Genocide', 'paragraphs': [{'context...      1.1\n",
       "4  {'title': 'Antibiotics', 'paragraphs': [{'cont...      1.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showQuestion(titleId, paragraphId, questionId):\n",
    "\n",
    "    title = df['data'][titleId]['title']\n",
    "    paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "    question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "    answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "\n",
    "    printBold('Title')\n",
    "    print(title)\n",
    "    printBold('Paragraph')\n",
    "    print(paragraph)\n",
    "    printBold('Question')\n",
    "    print(question)\n",
    "    printBold('Answer')\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles 490\n",
      "Paragraphs 20963\n",
      "Questions 98169\n"
     ]
    }
   ],
   "source": [
    "titlesCount = len(df['data'])\n",
    "totalParagraphsCount = 0\n",
    "totalQuestionsCount = 0\n",
    "\n",
    "for titleId in range(titlesCount):\n",
    "    paragraphsCount = len(df['data'][titleId]['paragraphs'])\n",
    "    totalParagraphsCount += paragraphsCount\n",
    "    \n",
    "    for paragraphId in range(paragraphsCount):\n",
    "        questionsCount = len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])\n",
    "        \n",
    "        totalQuestionsCount += questionsCount\n",
    "        \n",
    "print('Titles', titlesCount)\n",
    "print('Paragraphs', totalParagraphsCount)\n",
    "print('Questions', totalQuestionsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['University_of_Notre_Dame',\n",
       " 'Beyoncé',\n",
       " 'Montana',\n",
       " 'Genocide',\n",
       " 'Antibiotics',\n",
       " 'Frédéric_Chopin',\n",
       " 'Sino-Tibetan_relations_during_the_Ming_dynasty',\n",
       " 'IPod',\n",
       " 'The_Legend_of_Zelda:_Twilight_Princess',\n",
       " 'Spectre_(2015_film)',\n",
       " '2008_Sichuan_earthquake',\n",
       " 'New_York_City',\n",
       " 'To_Kill_a_Mockingbird',\n",
       " 'Solar_energy',\n",
       " 'Tajikistan',\n",
       " 'Anthropology',\n",
       " 'Portugal',\n",
       " 'Kanye_West',\n",
       " 'Buddhism',\n",
       " 'American_Idol',\n",
       " 'Dog',\n",
       " '2008_Summer_Olympics_torch_relay',\n",
       " 'Alfred_North_Whitehead',\n",
       " 'Financial_crisis_of_2007%E2%80%9308',\n",
       " 'Saint_Barth%C3%A9lemy',\n",
       " 'Genome',\n",
       " 'Comprehensive_school',\n",
       " 'Republic_of_the_Congo',\n",
       " 'Prime_minister',\n",
       " 'Institute_of_technology',\n",
       " 'Wayback_Machine',\n",
       " 'Dutch_Republic',\n",
       " 'Symbiosis',\n",
       " 'Canadian_Armed_Forces',\n",
       " 'Cardinal_(Catholicism)',\n",
       " 'Iranian_languages',\n",
       " 'Lighting',\n",
       " 'Separation_of_powers_under_the_United_States_Constitution',\n",
       " 'Architecture',\n",
       " 'Human_Development_Index',\n",
       " 'Southern_Europe',\n",
       " 'BBC_Television',\n",
       " 'Arnold_Schwarzenegger',\n",
       " 'Plymouth',\n",
       " 'Heresy',\n",
       " 'Warsaw_Pact',\n",
       " 'Materialism',\n",
       " 'Space_Race',\n",
       " 'Pub',\n",
       " 'Christian',\n",
       " 'Sony_Music_Entertainment',\n",
       " 'Oklahoma_City',\n",
       " 'Hunter-gatherer',\n",
       " 'United_Nations_Population_Fund',\n",
       " 'Russian_Soviet_Federative_Socialist_Republic',\n",
       " 'Universal_Studios',\n",
       " 'Alexander_Graham_Bell',\n",
       " 'Internet_service_provider',\n",
       " 'Comics',\n",
       " 'Saint_Helena',\n",
       " 'Aspirated_consonant',\n",
       " 'Hydrogen',\n",
       " 'Web_browser',\n",
       " 'Boston',\n",
       " 'BeiDou_Navigation_Satellite_System',\n",
       " 'Canon_law',\n",
       " 'Communications_in_Somalia',\n",
       " 'Catalan_language',\n",
       " 'Estonian_language',\n",
       " 'Paper',\n",
       " 'Arena_Football_League',\n",
       " 'Adult_contemporary_music',\n",
       " 'Matter',\n",
       " 'Westminster_Abbey',\n",
       " 'Nanjing',\n",
       " 'Bern',\n",
       " 'Daylight_saving_time',\n",
       " 'Royal_Institute_of_British_Architects',\n",
       " 'National_Archives_and_Records_Administration',\n",
       " 'Tristan_da_Cunha',\n",
       " 'University_of_Kansas',\n",
       " 'Political_corruption',\n",
       " 'Dialect',\n",
       " 'Classical_music',\n",
       " 'Slavs',\n",
       " 'Southampton',\n",
       " 'Treaty',\n",
       " 'Josip_Broz_Tito',\n",
       " 'Marshall_Islands',\n",
       " 'Szlachta',\n",
       " 'Virgil',\n",
       " 'Alps',\n",
       " 'Gene',\n",
       " 'Guinea-Bissau',\n",
       " 'List_of_numbered_streets_in_Manhattan',\n",
       " 'Brain',\n",
       " 'Near_East',\n",
       " 'Zhejiang',\n",
       " 'Ministry_of_Defence_(United_Kingdom)',\n",
       " 'High-definition_television',\n",
       " 'Wood',\n",
       " 'Somalis',\n",
       " 'Middle_Ages',\n",
       " 'Phonology',\n",
       " 'Computer',\n",
       " 'Black_people',\n",
       " 'The_Times',\n",
       " 'New_Delhi',\n",
       " 'Imamah_(Shia_doctrine)',\n",
       " 'Bird_migration',\n",
       " 'Atlantic_City,_New_Jersey',\n",
       " 'Immunology',\n",
       " 'MP3',\n",
       " 'House_music',\n",
       " 'Letter_case',\n",
       " 'Chihuahua_(state)',\n",
       " 'Pitch_(music)',\n",
       " 'England_national_football_team',\n",
       " 'Houston',\n",
       " 'Copper',\n",
       " 'Identity_(social_science)',\n",
       " 'Himachal_Pradesh',\n",
       " 'Communication',\n",
       " 'Grape',\n",
       " 'Computer_security',\n",
       " 'Orthodox_Judaism',\n",
       " 'Animal',\n",
       " 'Beer',\n",
       " 'Race_and_ethnicity_in_the_United_States_Census',\n",
       " 'United_States_dollar',\n",
       " 'Imperial_College_London',\n",
       " 'Gymnastics',\n",
       " 'Hanover',\n",
       " 'Emotion',\n",
       " 'FC_Barcelona',\n",
       " 'Everton_F.C.',\n",
       " 'Old_English',\n",
       " 'Aircraft_carrier',\n",
       " 'Federal_Aviation_Administration',\n",
       " 'Lancashire',\n",
       " 'Mesozoic',\n",
       " 'Videoconferencing',\n",
       " 'Gregorian_calendar',\n",
       " 'Xbox_360',\n",
       " 'Military_history_of_the_United_States',\n",
       " 'Hard_rock',\n",
       " 'Great_Plains',\n",
       " 'Infrared',\n",
       " 'Biodiversity',\n",
       " 'ASCII',\n",
       " 'Digestion',\n",
       " 'Federal_Bureau_of_Investigation',\n",
       " 'Adolescence',\n",
       " 'Antarctica',\n",
       " 'Mary_(mother_of_Jesus)',\n",
       " 'Melbourne',\n",
       " 'John,_King_of_England',\n",
       " 'Macintosh',\n",
       " 'Anti-aircraft_warfare',\n",
       " 'Sanskrit',\n",
       " 'Valencia',\n",
       " 'General_Electric',\n",
       " 'United_States_Army',\n",
       " 'Franco-Prussian_War',\n",
       " 'Eritrea',\n",
       " 'Uranium',\n",
       " 'Order_of_the_British_Empire',\n",
       " 'Age_of_Enlightenment',\n",
       " 'Circadian_rhythm',\n",
       " 'Elizabeth_II',\n",
       " 'Sexual_orientation',\n",
       " 'Dell',\n",
       " 'Capital_punishment_in_the_United_States',\n",
       " 'Nintendo_Entertainment_System',\n",
       " 'Ashkenazi_Jews',\n",
       " 'Athanasius_of_Alexandria',\n",
       " 'Seattle',\n",
       " 'Memory',\n",
       " 'Multiracial_American',\n",
       " 'Pharmaceutical_industry',\n",
       " 'Umayyad_Caliphate',\n",
       " 'Asphalt',\n",
       " 'Queen_Victoria',\n",
       " 'Freemasonry',\n",
       " 'Israel',\n",
       " 'Hellenistic_period',\n",
       " 'Napoleon',\n",
       " 'Bill_%26_Melinda_Gates_Foundation',\n",
       " 'Northwestern_University',\n",
       " 'Hokkien',\n",
       " 'Montevideo',\n",
       " 'Poultry',\n",
       " 'Arsenal_F.C.',\n",
       " 'Dutch_language',\n",
       " 'Buckingham_Palace',\n",
       " 'Incandescent_light_bulb',\n",
       " 'Clothing',\n",
       " 'Chicago_Cubs',\n",
       " 'States_of_Germany',\n",
       " 'Korean_War',\n",
       " 'Royal_Dutch_Shell',\n",
       " 'Copyright_infringement',\n",
       " 'Greece',\n",
       " 'Mammal',\n",
       " 'East_India_Company',\n",
       " 'Southeast_Asia',\n",
       " 'Professional_wrestling',\n",
       " 'Film_speed',\n",
       " 'Mexico_City',\n",
       " 'Germans',\n",
       " 'New_Haven,_Connecticut',\n",
       " 'Brigham_Young_University',\n",
       " 'Myocardial_infarction',\n",
       " 'Department_store',\n",
       " 'Intellectual_property',\n",
       " 'Florida',\n",
       " 'Queen_(band)',\n",
       " 'Presbyterianism',\n",
       " 'Thuringia',\n",
       " 'Predation',\n",
       " 'Marvel_Comics',\n",
       " 'British_Empire',\n",
       " 'Botany',\n",
       " 'Madonna_(entertainer)',\n",
       " 'London',\n",
       " 'Law_of_the_United_States',\n",
       " 'Myanmar',\n",
       " 'Jews',\n",
       " 'Cotton',\n",
       " 'Data_compression',\n",
       " 'The_Sun_(United_Kingdom)',\n",
       " 'Carnival',\n",
       " 'Pesticide',\n",
       " 'Somerset',\n",
       " 'Yale_University',\n",
       " 'Late_Middle_Ages',\n",
       " 'Ann_Arbor,_Michigan',\n",
       " 'Gothic_architecture',\n",
       " 'Cubism',\n",
       " 'Political_philosophy',\n",
       " 'Alloy',\n",
       " 'Norfolk_Island',\n",
       " 'Edmund_Burke',\n",
       " 'Samoa',\n",
       " 'Pope_Paul_VI',\n",
       " 'George_VI',\n",
       " 'Electric_motor',\n",
       " 'Switzerland',\n",
       " 'Mali',\n",
       " 'Nonprofit_organization',\n",
       " 'Raleigh,_North_Carolina',\n",
       " 'Nutrition',\n",
       " 'Crimean_War',\n",
       " 'Literature',\n",
       " 'Avicenna',\n",
       " 'Chinese_characters',\n",
       " 'Bermuda',\n",
       " 'Nigeria',\n",
       " 'Utrecht',\n",
       " 'John_von_Neumann',\n",
       " 'Molotov%E2%80%93Ribbentrop_Pact',\n",
       " 'Capacitor',\n",
       " 'History_of_science',\n",
       " 'Czech_language',\n",
       " 'Digimon',\n",
       " 'Glacier',\n",
       " 'Planck_constant',\n",
       " 'Comcast',\n",
       " 'Tuberculosis',\n",
       " 'Affirmative_action_in_the_United_States',\n",
       " 'FA_Cup',\n",
       " 'Alsace',\n",
       " 'Baptists',\n",
       " 'Child_labour',\n",
       " 'North_Carolina',\n",
       " 'Heian_period',\n",
       " 'On_the_Origin_of_Species',\n",
       " 'Dissolution_of_the_Soviet_Union',\n",
       " 'Crucifixion_of_Jesus',\n",
       " 'Miami',\n",
       " 'Supreme_court',\n",
       " 'Textual_criticism',\n",
       " 'Gramophone_record',\n",
       " 'Turner_Classic_Movies',\n",
       " 'Hindu_philosophy',\n",
       " 'Political_party',\n",
       " 'A_cappella',\n",
       " 'Dominican_Order',\n",
       " 'Eton_College',\n",
       " 'Cork_(city)',\n",
       " 'Federalism',\n",
       " 'Galicia_(Spain)',\n",
       " 'Green',\n",
       " 'USB',\n",
       " 'Sichuan',\n",
       " 'Unicode',\n",
       " 'Detroit',\n",
       " 'Culture',\n",
       " 'Sahara',\n",
       " 'Rule_of_law',\n",
       " 'Tibet',\n",
       " 'Exhibition_game',\n",
       " 'Strasbourg',\n",
       " 'Oklahoma',\n",
       " 'History_of_India',\n",
       " 'Gamal_Abdel_Nasser',\n",
       " 'Pope_John_XXIII',\n",
       " 'Time',\n",
       " 'European_Central_Bank',\n",
       " 'St._John%27s,_Newfoundland_and_Labrador',\n",
       " 'PlayStation_3',\n",
       " 'Royal_assent',\n",
       " 'Group_(mathematics)',\n",
       " 'Central_African_Republic',\n",
       " 'Asthma',\n",
       " 'LaserDisc',\n",
       " 'Annelid',\n",
       " 'God',\n",
       " 'War_on_Terror',\n",
       " 'Labour_Party_(UK)',\n",
       " 'Estonia',\n",
       " 'Serbo-Croatian',\n",
       " 'Alaska',\n",
       " 'Karl_Popper',\n",
       " 'Mandolin',\n",
       " 'Insect',\n",
       " 'Race_(human_categorization)',\n",
       " 'Paris',\n",
       " 'Apollo',\n",
       " 'United_States_presidential_election,_2004',\n",
       " 'IBM',\n",
       " 'Liberal_Party_of_Australia',\n",
       " 'Samurai',\n",
       " 'Software_testing',\n",
       " 'Glass',\n",
       " 'Renewable_energy_commercialization',\n",
       " 'Palermo',\n",
       " 'Zinc',\n",
       " 'Neoclassical_architecture',\n",
       " 'CBC_Television',\n",
       " 'Appalachian_Mountains',\n",
       " 'Energy',\n",
       " 'East_Prussia',\n",
       " 'Ottoman_Empire',\n",
       " 'Philosophy_of_space_and_time',\n",
       " 'Neolithic',\n",
       " 'Friedrich_Hayek',\n",
       " 'Diarrhea',\n",
       " 'Madrasa',\n",
       " 'Philadelphia',\n",
       " 'John_Kerry',\n",
       " 'Rajasthan',\n",
       " 'Guam',\n",
       " 'Empiricism',\n",
       " 'Idealism',\n",
       " 'Education',\n",
       " 'Tennessee',\n",
       " 'Post-punk',\n",
       " 'Canadian_football',\n",
       " 'Seven_Years%27_War',\n",
       " 'Richard_Feynman',\n",
       " 'Muammar_Gaddafi',\n",
       " 'Cyprus',\n",
       " 'Steven_Spielberg',\n",
       " 'Elevator',\n",
       " 'Neptune',\n",
       " 'Railway_electrification_system',\n",
       " 'Spanish_language_in_the_United_States',\n",
       " 'Charleston,_South_Carolina',\n",
       " 'Red',\n",
       " 'The_Blitz',\n",
       " 'Endangered_Species_Act',\n",
       " 'Vacuum',\n",
       " 'Han_dynasty',\n",
       " 'Greeks',\n",
       " 'Quran',\n",
       " 'Great_power',\n",
       " 'Geography_of_the_United_States',\n",
       " 'Compact_disc',\n",
       " 'Transistor',\n",
       " 'Modern_history',\n",
       " '51st_state',\n",
       " 'Antenna_(radio)',\n",
       " 'Flowering_plant',\n",
       " 'Hyderabad',\n",
       " 'Santa_Monica,_California',\n",
       " 'Washington_University_in_St._Louis',\n",
       " 'Central_Intelligence_Agency',\n",
       " 'Pain',\n",
       " 'Database',\n",
       " 'Tucson,_Arizona',\n",
       " 'Armenia',\n",
       " 'Bacteria',\n",
       " 'Printed_circuit_board',\n",
       " 'Premier_League',\n",
       " 'Roman_Republic',\n",
       " 'Pacific_War',\n",
       " 'Richmond,_Virginia',\n",
       " 'San_Diego',\n",
       " 'Muslim_world',\n",
       " 'Iran',\n",
       " 'British_Isles',\n",
       " 'Association_football',\n",
       " 'Georgian_architecture',\n",
       " 'Liberia',\n",
       " 'Windows_8',\n",
       " 'Swaziland',\n",
       " 'Translation',\n",
       " 'Airport',\n",
       " 'Kievan_Rus%27',\n",
       " 'Super_Nintendo_Entertainment_System',\n",
       " 'Sumer',\n",
       " 'Tuvalu',\n",
       " 'Immaculate_Conception',\n",
       " 'Namibia',\n",
       " 'Russian_language',\n",
       " 'United_States_Air_Force',\n",
       " 'Light-emitting_diode',\n",
       " 'Bird',\n",
       " 'Qing_dynasty',\n",
       " 'Indigenous_peoples_of_the_Americas',\n",
       " 'Egypt',\n",
       " 'Mosaic',\n",
       " 'University',\n",
       " 'Religion_in_ancient_Rome',\n",
       " 'YouTube',\n",
       " 'Separation_of_church_and_state_in_the_United_States',\n",
       " 'Protestantism',\n",
       " 'Bras%C3%ADlia',\n",
       " 'Economy_of_Greece',\n",
       " 'Party_leaders_of_the_United_States_House_of_Representatives',\n",
       " 'Armenians',\n",
       " 'Jehovah%27s_Witnesses',\n",
       " 'Dwight_D._Eisenhower',\n",
       " 'The_Bronx',\n",
       " 'Humanism',\n",
       " 'Geological_history_of_Earth',\n",
       " 'Police',\n",
       " 'Punjab,_Pakistan',\n",
       " 'Infection',\n",
       " 'Hunting',\n",
       " 'Kathmandu',\n",
       " 'Super_Bowl_50',\n",
       " 'Warsaw',\n",
       " 'Normans',\n",
       " 'Nikola_Tesla',\n",
       " 'Computational_complexity_theory',\n",
       " 'Teacher',\n",
       " 'Martin_Luther',\n",
       " 'Southern_California',\n",
       " 'Sky_(United_Kingdom)',\n",
       " 'Victoria_(Australia)',\n",
       " 'Huguenot',\n",
       " 'Steam_engine',\n",
       " 'Oxygen',\n",
       " '1973_oil_crisis',\n",
       " 'Apollo_program',\n",
       " 'European_Union_law',\n",
       " 'Amazon_rainforest',\n",
       " 'Ctenophora',\n",
       " 'Fresno,_California',\n",
       " 'Packet_switching',\n",
       " 'Black_Death',\n",
       " 'Geology',\n",
       " 'Newcastle_upon_Tyne',\n",
       " 'Victoria_and_Albert_Museum',\n",
       " 'American_Broadcasting_Company',\n",
       " 'Genghis_Khan',\n",
       " 'Pharmacy',\n",
       " 'Immune_system',\n",
       " 'Civil_disobedience',\n",
       " 'Construction',\n",
       " 'Private_school',\n",
       " 'Harvard_University',\n",
       " 'Jacksonville,_Florida',\n",
       " 'Economic_inequality',\n",
       " 'Doctor_Who',\n",
       " 'University_of_Chicago',\n",
       " 'Yuan_dynasty',\n",
       " 'Kenya',\n",
       " 'Intergovernmental_Panel_on_Climate_Change',\n",
       " 'Chloroplast',\n",
       " 'Prime_number',\n",
       " 'Rhine',\n",
       " 'Scottish_Parliament',\n",
       " 'Islamism',\n",
       " 'Imperialism',\n",
       " 'United_Methodist_Church',\n",
       " 'French_and_Indian_War',\n",
       " 'Force']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "for titleId in range(len(df['data'])):\n",
    "    titles.append(df['data'][titleId]['title'])\n",
    "    \n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are pretty random. Seems to be a lot of locations like countries and cities but not nearly enough to afford splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our main assumptions is that the sentence that contains the answer could be turned into a question just by removing the answer from it. Let's see how much of that is true for the questions in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def containedInText()\n",
    "    #for each word in the question check if it's contained in the sentence\n",
    "    \n",
    "#def extractSentence()\n",
    "\n",
    "#or the parapragh\n",
    "\n",
    "#return fraction for containment in the range [0;1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentence(paragrapgh, answerStart):\n",
    "    sentences = tokenize.sent_tokenize(paragrapgh)\n",
    "    \n",
    "    sentenceStart = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if (sentenceStart + len(sentence) >= answerStart):\n",
    "            return sentence         \n",
    "        \n",
    "        sentenceStart += len(sentence) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    }
   ],
   "source": [
    "paragrapgh = df['data'][0]['paragraphs'][0]['context']\n",
    "answerStart = df['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "\n",
    "sentence = extractSentence(paragrapgh, answerStart)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containedInText(text, question):\n",
    "    \n",
    "    questionWords = tokenize.word_tokenize(question.lower())\n",
    "    textWords = tokenize.word_tokenize(text.lower())\n",
    "    wordsContained = 0\n",
    "\n",
    "    for questionWord in questionWords:\n",
    "        for textWord in textWords:\n",
    "            if (questionWord == textWord):\n",
    "                wordsContained += 1\n",
    "                break\n",
    "\n",
    "    return wordsContained / len(questionWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  df['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "\n",
    "contained = containedInText(sentence, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Contained**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "printBold('Question')\n",
    "print(question)\n",
    "printBold('Sentence')\n",
    "print(sentence)\n",
    "printBold(\"Contained\")\n",
    "print(contained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't expect a 100% containment simply because the questions will contain **question-like words** like *Why, Who, *Whom*, What*.\n",
    "\n",
    "In this example we also see that the word appear is contained in the original sentence but in **past tense**. We could take care of that if we take the **stems** of the words, but I think it's better to see the least imaginative way for forming questions.\n",
    "\n",
    "We are also calculating some **common words like *to, the, in*** which could be encountered at different places of the sentence, but again we want to measure the least-creative questions.\n",
    "\n",
    "In this sentece *(damn, that was a good example)* we also see that the question uses the word *allegedly* which is a **synonym** of *reputedly* in the sentence. That could be nice for question forming, but I think it's more of an overkill.\n",
    "\n",
    "We also see that the question actually encompasses the **words around the answer, rather than the entire sentence**. Which is a definate must-do when we form our questions. \n",
    "\n",
    "Let's see what is the score on all of the questons. I'm also curious to see the score on the entire paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n"
     ]
    }
   ],
   "source": [
    "sentenceScore = []\n",
    "paragrapghScore = []\n",
    "\n",
    "\n",
    "#For each title\n",
    "for titleId in range(len(df['data'])):\n",
    "    print(titleId)\n",
    "    #For each paragraph\n",
    "    for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragrapgh = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        #For each question\n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "            answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "            sentence = extractSentence(paragrapgh, answerStart)\n",
    "          \n",
    "            sentenceScore.append(containedInText(sentence, question))\n",
    "            paragrapghScore.append(containedInText(paragrapgh, question))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=98169, minmax=(0.0, 1.0), mean=0.4639372890613787, variance=0.036243536466669425, skewness=-0.1260186187793197, kurtosis=-0.49616712681619424)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(sentenceScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=98169, minmax=(0.0, 1.0), mean=0.5821566303382768, variance=0.025298457602856428, skewness=-0.37555623686086964, kurtosis=-0.07449784279571592)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(paragrapghScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that almost half the words contained is a pretty good result. \n",
    "\n",
    "As expected, contained within the entire paragrapgh is better.\n",
    "\n",
    "I do wonder about those questions that are 100% contained in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
