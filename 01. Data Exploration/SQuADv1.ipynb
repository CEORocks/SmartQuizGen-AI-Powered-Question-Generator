{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration - SQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from nltk import tokenize\n",
    "from scipy import stats\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBold(string):\n",
    "    display(Markdown('**' + string + '**'))\n",
    "    \n",
    "#def printColor():\n",
    "#     display(Markdown('<span style=\"color:blue\">blue</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aren't really doing the answering of the questions, as is the true intention for the dataset, we'll merge the train and dev datasets into one. The test dataset is probably hidden, since there's a competition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/squad-v1/train-v1.1.json', orient='column')\n",
    "dev = pd.read_json('../data/squad-v1/dev-v1.1.json', orient='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, dev], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'University_of_Notre_Dame', 'paragra...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Beyoncé', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Montana', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Genocide', 'paragraphs': [{'context...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Antibiotics', 'paragraphs': [{'cont...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  version\n",
       "0  {'title': 'University_of_Notre_Dame', 'paragra...      1.1\n",
       "1  {'title': 'Beyoncé', 'paragraphs': [{'context'...      1.1\n",
       "2  {'title': 'Montana', 'paragraphs': [{'context'...      1.1\n",
       "3  {'title': 'Genocide', 'paragraphs': [{'context...      1.1\n",
       "4  {'title': 'Antibiotics', 'paragraphs': [{'cont...      1.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showQuestion(titleId, paragraphId, questionId):\n",
    "\n",
    "    title = df['data'][titleId]['title']\n",
    "    paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "    question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "    answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "    answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "\n",
    "    printBold('Title')\n",
    "    print(title)\n",
    "    printBold('Paragraph')\n",
    "    print(paragraph)\n",
    "    printBold('Question')\n",
    "    print(question)\n",
    "    printBold('Answer')\n",
    "    print(answerStart)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles 490\n",
      "Paragraphs 20963\n",
      "Questions 98169\n"
     ]
    }
   ],
   "source": [
    "titlesCount = len(df['data'])\n",
    "totalParagraphsCount = 0\n",
    "totalQuestionsCount = 0\n",
    "\n",
    "for titleId in range(titlesCount):\n",
    "    paragraphsCount = len(df['data'][titleId]['paragraphs'])\n",
    "    totalParagraphsCount += paragraphsCount\n",
    "    \n",
    "    for paragraphId in range(paragraphsCount):\n",
    "        questionsCount = len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])\n",
    "        \n",
    "        totalQuestionsCount += questionsCount\n",
    "        \n",
    "print('Titles', titlesCount)\n",
    "print('Paragraphs', totalParagraphsCount)\n",
    "print('Questions', totalQuestionsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n",
      "Beyoncé\n",
      "Montana\n",
      "Genocide\n",
      "Antibiotics\n",
      "Frédéric_Chopin\n",
      "Sino-Tibetan_relations_during_the_Ming_dynasty\n",
      "IPod\n",
      "The_Legend_of_Zelda:_Twilight_Princess\n",
      "Spectre_(2015_film)\n",
      "2008_Sichuan_earthquake\n",
      "New_York_City\n",
      "To_Kill_a_Mockingbird\n",
      "Solar_energy\n",
      "Tajikistan\n",
      "Anthropology\n",
      "Portugal\n",
      "Kanye_West\n",
      "Buddhism\n",
      "American_Idol\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "for titleId in range(len(df['data'])):\n",
    "    titles.append(df['data'][titleId]['title'])\n",
    "    \n",
    "for i in range(20):\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are pretty random. Seems to be a lot of locations like countries and cities but not nearly enough to afford splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our main assumptions is that the sentence that contains the answer could be turned into a question just by removing the answer from it. Let's see how much of that is true for the questions in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentence(paragraph, answerStart):\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(paragraph)\n",
    "    sentenceStart = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if (sentenceStart + len(sentence) >= answerStart):\n",
    "            return sentence         \n",
    "        \n",
    "        sentenceStart += len(sentence) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    }
   ],
   "source": [
    "paragraph = df['data'][0]['paragraphs'][0]['context']\n",
    "answerStart = df['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "\n",
    "sentence = extractSentence(paragraph, answerStart)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containedInText(text, question):\n",
    "    \n",
    "    questionWords = tokenize.word_tokenize(question.lower())\n",
    "    textWords = tokenize.word_tokenize(text.lower())\n",
    "    wordsContained = 0\n",
    "\n",
    "    for questionWord in questionWords:\n",
    "        for textWord in textWords:\n",
    "            if (questionWord == textWord):\n",
    "                wordsContained += 1\n",
    "                break\n",
    "\n",
    "    return wordsContained / len(questionWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  df['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "\n",
    "contained = containedInText(sentence, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Contained**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "printBold('Question')\n",
    "print(question)\n",
    "printBold('Sentence')\n",
    "print(sentence)\n",
    "printBold(\"Contained\")\n",
    "print(contained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't expect a 100% containment simply because the questions will contain **question-like words** like *Why, Who, *Whom*, What*.\n",
    "\n",
    "In this example we also see that the word appear is contained in the original sentence but in **past tense**. We could take care of that if we take the **stems** of the words, but I think it's better to see the least imaginative way for forming questions.\n",
    "\n",
    "We are also calculating some **common words like *to, the, in*** which could be encountered at different places of the sentence, but again we want to measure the least-creative questions.\n",
    "\n",
    "In this sentece *(damn, that was a good example)* we also see that the question uses the word *allegedly* which is a **synonym** of *reputedly* in the sentence. That could be nice for question forming, but I think it's more of an overkill.\n",
    "\n",
    "We also see that the question actually encompasses the **words around the answer, rather than the entire sentence**. Which is a definate must-do when we form our questions. \n",
    "\n",
    "Let's see what is the score on all of the questons. I'm also curious to see the score on the entire paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may come in handy in the future. Pretty printing the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printint the percentage done\n",
    "def printPercentage(currentStep, maxStep):\n",
    "    stepSize = maxStep / 100\n",
    "    \n",
    "    if (int(currentStep / stepSize) > ((currentStep - 1) / stepSize)):\n",
    "        clear_output()\n",
    "        print('{}%'.format(int(currentStep / stepSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "sentenceScore = []\n",
    "paragraphScore = []\n",
    "\n",
    "#For each title\n",
    "titlesCount = len(df['data'])\n",
    "for titleId in range(titlesCount):\n",
    "    printPercentage(titleId, titlesCount)\n",
    "    \n",
    "    #For each paragraph\n",
    "    for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        \n",
    "        #For each question\n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "            answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "            sentence = extractSentence(paragraph, answerStart)\n",
    "          \n",
    "            sentenceScore.append(containedInText(sentence, question))\n",
    "            paragraphScore.append(containedInText(paragraph, question))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98169.000000</td>\n",
       "      <td>98169.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.463937</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.190377</td>\n",
       "      <td>0.159055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence     paragraph\n",
       "count  98169.000000  98169.000000\n",
       "mean       0.463937      0.582157\n",
       "std        0.190377      0.159055\n",
       "min        0.000000      0.000000\n",
       "25%        0.333333      0.500000\n",
       "50%        0.461538      0.600000\n",
       "75%        0.600000      0.700000\n",
       "max        1.000000      1.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceScoreDf = pd.DataFrame(sentenceScore, columns=['sentence'])\n",
    "paragraphScoreDf = pd.DataFrame(paragraphScore, columns=['paragraph'])\n",
    "\n",
    "questionContainmentDf = pd.concat([sentenceScoreDf, paragraphScoreDf], axis=1)\n",
    "questionContainmentDf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that almost half the words contained is a pretty good result. \n",
    "\n",
    "As expected, contained within the entire paragraph is better.\n",
    "\n",
    "I do wonder about those questions that are 100% contained in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence  paragraph\n",
       "0  0.642857   0.571429\n",
       "1  0.636364   0.636364\n",
       "2  0.533333   0.600000\n",
       "3  0.375000   0.500000\n",
       "4  0.333333   0.416667\n",
       "5  0.272727   0.636364\n",
       "6  0.300000   0.800000\n",
       "7  0.363636   0.727273\n",
       "8  0.000000   0.545455\n",
       "9  0.266667   0.733333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionAt(index):\n",
    "    currentIndex = 0\n",
    "    \n",
    "    for titleId in range(len(df['data'])):\n",
    "        for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "            for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "                if (currentIndex == index):\n",
    "                    return titleId, paragraphId, questionId\n",
    "                currentIndex += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see question #8 which has 0 containment in the answer sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many student news papers are found at Notre Dame?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "three\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 1 \n",
    "questionId = 3\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is actually formed from the previous sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence  paragraph\n",
       "269        0.0        0.0\n",
       "363        0.0        0.0\n",
       "505        0.0        0.0\n",
       "2781       0.0        0.0\n",
       "3678       0.0        0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf[questionContainmentDf['paragraph'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "titleId = 1\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **synonym** case - *instead of rose to fame*, *start becoming popular* is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2011, documents obtained by WikiLeaks revealed that Beyoncé was one of many entertainers who performed for the family of Libyan ruler Muammar Gaddafi. Rolling Stone reported that the music industry was urging them to return the money they earned for the concerts; a spokesperson for Beyoncé later confirmed to The Huffington Post that she donated the money to the Clinton Bush Haiti Fund. Later that year she became the first solo female artist to headline the main Pyramid stage at the 2011 Glastonbury Festival in over twenty years, and was named the highest-paid performer in the world per minute.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did this leak happen?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2011\n"
     ]
    }
   ],
   "source": [
    "titleId = 1\n",
    "paragraphId = 18 \n",
    "questionId = 6\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's just a bad question. It could only be asked in combination with the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21911</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39394</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45064</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48874</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53226</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67425</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  paragraph\n",
       "21911       1.0        1.0\n",
       "39394       1.0        1.0\n",
       "45064       1.0        1.0\n",
       "48874       1.0        1.0\n",
       "53226       1.0        1.0\n",
       "67425       1.0        1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf[questionContainmentDf['sentence'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258, 23, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(53226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utrecht\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam. There are several theatres and theatre companies. The 1941 main city theatre was built by Dudok. Besides theatres there is a large number of cinemas including three arthouse cinemas. Utrecht is host to the international Early Music Festival (Festival Oude Muziek, for music before 1800) and the Netherlands Film Festival. The city has an important classical music hall Vredenburg (1979 by Herman Hertzberger). Its acoustics are considered among the best of the 20th-century original music halls.[citation needed] The original Vredenburg music hall has been redeveloped as part of the larger station area redevelopment plan and in 2014 has gained additional halls that allowed its merger with the rock club Tivoli and the SJU jazzpodium. There are several other venues for music throughout the city. Young musicians are educated in the conservatory, a department of the Utrecht School of the Arts. There is a specialised museum of automatically playing musical instruments.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cultural life in Utrecht is second to \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam\n"
     ]
    }
   ],
   "source": [
    "titleId = 258\n",
    "paragraphId = 23 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange question. The question words all appear in the sentence, but not in order. But the answer is the entire sentence, which obviously has needless information inside it. Looking further into it, the question is actually wrong, because it should state second *in Netherlands*. This question should be scrapped..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 25, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(67425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A reversible process is one in which this does not happen.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "dissipation\n"
     ]
    }
   ],
   "source": [
    "titleId = 341\n",
    "paragraphId = 25 \n",
    "questionId = 2\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, basically, just the question I expect to generate. The answer is removed and the sentence is descriptive enough to fill in the missing word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that the **question is mostly consisted of words from the sentence the answer is in** seems correct.\n",
    "\n",
    "There are some obvious differences like:\n",
    "- **Question-like words** are added - who, why, when...\n",
    "- **Synonyms** are used instead of the words used in the sentence\n",
    "- Changing the sentence to a question also changes the **tense** of the word.\n",
    "- In long sentences, only a **part of the sentence is used**. Like if the sentence is separated with commas, the comma actually divides two logical statements.\n",
    "\n",
    "I also managed to find some outliers which turned out to be not-so-well asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see:\n",
    "- Are all the answers phrases from the text\n",
    "- The type of the answers - number, dates, names, locations, close to the title\n",
    "- Part of speech - verb, noun\n",
    "- Answer lenght in words\n",
    "- Words around the answer.\n",
    "- Answer location in the sentence - First word, last word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers contained in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answers in text**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98169\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answers not in text**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "answersInText = 0\n",
    "answersNotInText = 0\n",
    "\n",
    "for titleId in range(len(df['data'])):\n",
    "     for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "            if (answer in paragraph):\n",
    "                answersInText += 1\n",
    "            else:\n",
    "                answersNotInText += 1\n",
    "                \n",
    "printBold('Answers in text')\n",
    "print(answersInText)\n",
    "printBold('Answers not in text')\n",
    "print(answersNotInText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the answers are phrases from the text. Seems like that has been a requirement from the start, since the answers also have an index indicating their start location in the paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "sentences = []\n",
    "\n",
    "for titleId in range(len(df['data'])):\n",
    "    \n",
    "     for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        \n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "            answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "            \n",
    "            sentence = extractSentence(paragraph, answerStart)\n",
    "            \n",
    "            answers.append(answer)\n",
    "            sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  \n",
       "0  It is a replica of the grotto at Lourdes, Fran...  \n",
       "1  Immediately in front of the Main Building and ...  \n",
       "2  Next to the Main Building is the Basilica of t...  \n",
       "3  Immediately behind the basilica is the Grotto,...  \n",
       "4  Atop the Main Building's gold dome is a golden...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerTextsDf = pd.DataFrame(answers, columns=['answer'])\n",
    "sentenceDf = pd.DataFrame(sentences, columns=['sentence'])\n",
    "\n",
    "answersDf = pd.concat([answerTextsDf, sentenceDf], axis=1)\n",
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer word lenght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = []\n",
    "\n",
    "for i in range(len(answersDf)):\n",
    "    wordCount.append(len(tokenize.word_tokenize(answersDf.iloc[i]['answer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDf = pd.concat([answersDf, pd.DataFrame(wordCount, columns=['wordCount'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    98169.000000\n",
       "mean         3.354511\n",
       "std          3.731074\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max         46.000000\n",
       "Name: wordCount, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     32161\n",
       "2     25233\n",
       "3     14350\n",
       "4      7557\n",
       "5      4654\n",
       "6      3050\n",
       "7      2222\n",
       "8      1676\n",
       "9      1206\n",
       "10      974\n",
       "11      755\n",
       "12      653\n",
       "13      565\n",
       "14      462\n",
       "15      406\n",
       "16      313\n",
       "18      275\n",
       "17      269\n",
       "19      243\n",
       "20      191\n",
       "21      182\n",
       "23      138\n",
       "22      132\n",
       "25      120\n",
       "24      101\n",
       "26       78\n",
       "28       58\n",
       "27       57\n",
       "29       29\n",
       "30       18\n",
       "31       12\n",
       "32       11\n",
       "33        6\n",
       "38        2\n",
       "34        2\n",
       "35        2\n",
       "36        2\n",
       "37        2\n",
       "42        1\n",
       "46        1\n",
       "Name: wordCount, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1/3 of of the answers are single words. And about 2/3 are up to 3 words. Let's get an overview of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52642</th>\n",
       "      <td>mul</td>\n",
       "      <td>For example, the name for the hanja 水 is 물 수 (...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79457</th>\n",
       "      <td>11,000–16,000</td>\n",
       "      <td>The total Iranian casualties in the war were e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88678</th>\n",
       "      <td>Saracens</td>\n",
       "      <td>From these bases, the Normans eventually captu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35390</th>\n",
       "      <td>microphone</td>\n",
       "      <td>The second controller lacked the START and SEL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34469</th>\n",
       "      <td>rarely</td>\n",
       "      <td>Since Elizabeth rarely gives interviews, littl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57333</th>\n",
       "      <td>1991</td>\n",
       "      <td>By the late 1980s, digital media, in the form ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10684</th>\n",
       "      <td>ZigBee</td>\n",
       "      <td>Many newer control systems are using wireless ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43080</th>\n",
       "      <td>1990s</td>\n",
       "      <td>Intergender singles bouts were first fought on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43755</th>\n",
       "      <td>1870</td>\n",
       "      <td>In 1870, after France attacked Prussia, Prussi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65525</th>\n",
       "      <td>Champs-Élysées</td>\n",
       "      <td>As of 2013 the City of Paris had 1,570 hotels ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               answer                                           sentence  \\\n",
       "52642             mul  For example, the name for the hanja 水 is 물 수 (...   \n",
       "79457   11,000–16,000  The total Iranian casualties in the war were e...   \n",
       "88678        Saracens  From these bases, the Normans eventually captu...   \n",
       "35390      microphone  The second controller lacked the START and SEL...   \n",
       "34469          rarely  Since Elizabeth rarely gives interviews, littl...   \n",
       "57333            1991  By the late 1980s, digital media, in the form ...   \n",
       "10684          ZigBee  Many newer control systems are using wireless ...   \n",
       "43080           1990s  Intergender singles bouts were first fought on...   \n",
       "43755            1870  In 1870, after France attacked Prussia, Prussi...   \n",
       "65525  Champs-Élysées  As of 2013 the City of Paris had 1,570 hotels ...   \n",
       "\n",
       "       wordCount  \n",
       "52642          1  \n",
       "79457          1  \n",
       "88678          1  \n",
       "35390          1  \n",
       "34469          1  \n",
       "57333          1  \n",
       "10684          1  \n",
       "43080          1  \n",
       "43755          1  \n",
       "65525          1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 1].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of years and some names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word answers seem to be dominated by names. There are also a lot of answers where one of the words isn't useful. Some could easily be removed like *a* and *the*. *six years* and *tree times* could also be turned to just 6 and 3. The *13.3%* seems to be just misplaced. Not sure if it's because of the *\".\"* or the *\"%\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31777</th>\n",
       "      <td>six years</td>\n",
       "      <td>A peace agreement was signed in which John ret...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>Notre Dame</td>\n",
       "      <td>In 2006, Lee was awarded an honorary doctorate...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21766</th>\n",
       "      <td>gamma-aminobutyric acid</td>\n",
       "      <td>The two neurotransmitters that are used most w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28267</th>\n",
       "      <td>Thomas Aquinas</td>\n",
       "      <td>During the Middle Ages, the Aristotelian view ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>The single, \"A Moment Like This\", went on to b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26176</th>\n",
       "      <td>migratory species</td>\n",
       "      <td>The state is also a host to a large population...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33975</th>\n",
       "      <td>over five</td>\n",
       "      <td>For example, over five columns of text were de...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>Mockingbird groupies</td>\n",
       "      <td>Local residents call them \"Mockingbird groupie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85540</th>\n",
       "      <td>Alan Rogerson</td>\n",
       "      <td>Former members Heather and Gary Botting compar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77579</th>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>The first ever Premier League goal was scored ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33022</th>\n",
       "      <td>suppressive fire</td>\n",
       "      <td>Many units are supplemented with a variety of ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19486</th>\n",
       "      <td>political boundaries</td>\n",
       "      <td>This claim also cannot be used to invalidate t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92917</th>\n",
       "      <td>20 minutes</td>\n",
       "      <td>It is connected to the city via the Metro Ligh...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56025</th>\n",
       "      <td>Ashe County</td>\n",
       "      <td>Over the last decade, North Carolina has becom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28701</th>\n",
       "      <td>Brian Labone</td>\n",
       "      <td>The late centre half and former captain Brian ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>learning investments</td>\n",
       "      <td>Hence the additional costs of the incentives f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22254</th>\n",
       "      <td>23.02%</td>\n",
       "      <td>According to surveys conducted in 2007 and 200...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>Jennifer Hudson</td>\n",
       "      <td>Other alumni have gone on to work in televisio...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54428</th>\n",
       "      <td>every continent</td>\n",
       "      <td>Glaciers are present on every continent and ap...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29432</th>\n",
       "      <td>21 March</td>\n",
       "      <td>The Church of Alexandria celebrated Easter on ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        answer  \\\n",
       "31777                six years   \n",
       "4799                Notre Dame   \n",
       "21766  gamma-aminobutyric acid   \n",
       "28267           Thomas Aquinas   \n",
       "7152               The Beatles   \n",
       "26176        migratory species   \n",
       "33975                over five   \n",
       "4851      Mockingbird groupies   \n",
       "85540            Alan Rogerson   \n",
       "77579         Sheffield United   \n",
       "33022         suppressive fire   \n",
       "19486     political boundaries   \n",
       "92917               20 minutes   \n",
       "56025              Ashe County   \n",
       "28701             Brian Labone   \n",
       "5075      learning investments   \n",
       "22254                   23.02%   \n",
       "7646           Jennifer Hudson   \n",
       "54428          every continent   \n",
       "29432                 21 March   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "31777  A peace agreement was signed in which John ret...          2  \n",
       "4799   In 2006, Lee was awarded an honorary doctorate...          2  \n",
       "21766  The two neurotransmitters that are used most w...          2  \n",
       "28267  During the Middle Ages, the Aristotelian view ...          2  \n",
       "7152   The single, \"A Moment Like This\", went on to b...          2  \n",
       "26176  The state is also a host to a large population...          2  \n",
       "33975  For example, over five columns of text were de...          2  \n",
       "4851   Local residents call them \"Mockingbird groupie...          2  \n",
       "85540  Former members Heather and Gary Botting compar...          2  \n",
       "77579  The first ever Premier League goal was scored ...          2  \n",
       "33022  Many units are supplemented with a variety of ...          2  \n",
       "19486  This claim also cannot be used to invalidate t...          2  \n",
       "92917  It is connected to the city via the Metro Ligh...          2  \n",
       "56025  Over the last decade, North Carolina has becom...          2  \n",
       "28701  The late centre half and former captain Brian ...          2  \n",
       "5075   Hence the additional costs of the incentives f...          2  \n",
       "22254  According to surveys conducted in 2007 and 200...          2  \n",
       "7646   Other alumni have gone on to work in televisio...          2  \n",
       "54428  Glaciers are present on every continent and ap...          2  \n",
       "29432  The Church of Alexandria celebrated Easter on ...          2  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 2].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word answers seem to be dominated by names. There are also a lot of answers where one of the words isn't useful. Some could easily be removed like *a*, *in* and *the*. *six years* could be turned to just 6. The *23.02%* seems to be just misplaced. Not sure if it's because of the *\".\"* or the *\"%\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>Vasco da Gama</td>\n",
       "      <td>Portugal had during the 15th century – particu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28486</th>\n",
       "      <td>Copa del Generalísimo</td>\n",
       "      <td>The 1960s saw the emergence of Josep Maria Fus...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91654</th>\n",
       "      <td>magnetic tape shortage</td>\n",
       "      <td>During the following years, a magnetic tape sh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95828</th>\n",
       "      <td>fear of betrayal</td>\n",
       "      <td>In 1354, when Toghtogha led a large army to cr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61090</th>\n",
       "      <td>Arab Umayyad Caliphate</td>\n",
       "      <td>After conquering Persia, the Arab Umayyad Cali...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92998</th>\n",
       "      <td>keyed Northumbrian smallpipes</td>\n",
       "      <td>John Dunn, inventor of keyed Northumbrian smal...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66068</th>\n",
       "      <td>The Weather Company</td>\n",
       "      <td>On October 28, 2015, IBM announced its acquisi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543</th>\n",
       "      <td>10 February 1931</td>\n",
       "      <td>The city that was later dubbed \"Lutyens' Delhi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50145</th>\n",
       "      <td>political and moral</td>\n",
       "      <td>He is without parallel in any age, excepting p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84203</th>\n",
       "      <td>the Roku player</td>\n",
       "      <td>Google made YouTube available on the Roku play...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72640</th>\n",
       "      <td>1792 and 1793</td>\n",
       "      <td>The guillotines used during the Reign of Terro...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78086</th>\n",
       "      <td>power to veto</td>\n",
       "      <td>This made his person sacrosanct, gave him the ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85670</th>\n",
       "      <td>George C. Marshall</td>\n",
       "      <td>Next, he was appointed Assistant Chief of Staf...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>Pius V.</td>\n",
       "      <td>The use of the title was reserved for the card...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49201</th>\n",
       "      <td>Frederick the Wise</td>\n",
       "      <td>When he refused, he was placed under the ban o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>Reporters Without Borders</td>\n",
       "      <td>Reporters Without Borders organised several sy...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82432</th>\n",
       "      <td>mythical chullumpi bird</td>\n",
       "      <td>The mythical chullumpi bird is said to mark th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44100</th>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>New Haven is the birthplace of former presiden...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93328</th>\n",
       "      <td>president and CEO</td>\n",
       "      <td>Noble subsequently acquired the rights to the ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61195</th>\n",
       "      <td>East India Company</td>\n",
       "      <td>This led to the Battle of Plassey on 23 June 1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              answer  \\\n",
       "49157                  Vasco da Gama   \n",
       "28486          Copa del Generalísimo   \n",
       "91654         magnetic tape shortage   \n",
       "95828               fear of betrayal   \n",
       "61090         Arab Umayyad Caliphate   \n",
       "92998  keyed Northumbrian smallpipes   \n",
       "66068            The Weather Company   \n",
       "24543               10 February 1931   \n",
       "50145            political and moral   \n",
       "84203                the Roku player   \n",
       "72640                  1792 and 1793   \n",
       "78086                  power to veto   \n",
       "85670             George C. Marshall   \n",
       "10430                        Pius V.   \n",
       "49201             Frederick the Wise   \n",
       "8290       Reporters Without Borders   \n",
       "82432        mythical chullumpi bird   \n",
       "44100                 George W. Bush   \n",
       "93328              president and CEO   \n",
       "61195             East India Company   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "49157  Portugal had during the 15th century – particu...          3  \n",
       "28486  The 1960s saw the emergence of Josep Maria Fus...          3  \n",
       "91654  During the following years, a magnetic tape sh...          3  \n",
       "95828  In 1354, when Toghtogha led a large army to cr...          3  \n",
       "61090  After conquering Persia, the Arab Umayyad Cali...          3  \n",
       "92998  John Dunn, inventor of keyed Northumbrian smal...          3  \n",
       "66068  On October 28, 2015, IBM announced its acquisi...          3  \n",
       "24543  The city that was later dubbed \"Lutyens' Delhi...          3  \n",
       "50145  He is without parallel in any age, excepting p...          3  \n",
       "84203  Google made YouTube available on the Roku play...          3  \n",
       "72640  The guillotines used during the Reign of Terro...          3  \n",
       "78086  This made his person sacrosanct, gave him the ...          3  \n",
       "85670  Next, he was appointed Assistant Chief of Staf...          3  \n",
       "10430  The use of the title was reserved for the card...          3  \n",
       "49201  When he refused, he was placed under the ban o...          3  \n",
       "8290   Reporters Without Borders organised several sy...          3  \n",
       "82432  The mythical chullumpi bird is said to mark th...          3  \n",
       "44100  New Haven is the birthplace of former presiden...          3  \n",
       "93328  Noble subsequently acquired the rights to the ...          3  \n",
       "61195  This led to the Battle of Plassey on 23 June 1...          3  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 3].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again names, more institution names as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85209</th>\n",
       "      <td>conduct surveys of party colleagues</td>\n",
       "      <td>For instance, to keep their party colleagues \"...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>Robert Bideleux and Ian Jeffries</td>\n",
       "      <td>Significant legislative changes in the status ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22367</th>\n",
       "      <td>in excess of £3.3 billion</td>\n",
       "      <td>The total annual cost to support the defence e...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64526</th>\n",
       "      <td>Koninklijk Conservatorium Artesis Hogeschool A...</td>\n",
       "      <td>She is now also professor mandolin at the musi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97009</th>\n",
       "      <td>end of World War I</td>\n",
       "      <td>At the end of World War I, the Rhineland was s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54390</th>\n",
       "      <td>partly cold-based and partly warm-based</td>\n",
       "      <td>Glaciers which are partly cold-based and partl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90084</th>\n",
       "      <td>body and blood of Christ</td>\n",
       "      <td>Luther insisted on the Real Presence of the bo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39942</th>\n",
       "      <td>the eastern waterfront in Buceo</td>\n",
       "      <td>The Museo Naval, is located on the eastern wat...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95598</th>\n",
       "      <td>School of Social Service Administration</td>\n",
       "      <td>In 1955, Eero Saarinen was contracted to devel...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58652</th>\n",
       "      <td>protruded from the road surface</td>\n",
       "      <td>However, the company ceased trading in 1875 af...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56345</th>\n",
       "      <td>scientific naturalism over natural theology</td>\n",
       "      <td>Huxley wanted science to be secular, without r...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25781</th>\n",
       "      <td>Kraftwerk, Art of Noise</td>\n",
       "      <td>This sound, also influenced by European electr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38132</th>\n",
       "      <td>a great-great grandson of Jacob</td>\n",
       "      <td>Jacob and his sons had lived in Canaan but wer...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41885</th>\n",
       "      <td>off Australia's northwestern coast</td>\n",
       "      <td>On 20 May 2011, Royal Dutch Shell's final inve...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64614</th>\n",
       "      <td>Chris Thile of California is</td>\n",
       "      <td>Chris Thile of California is a well-known play...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89455</th>\n",
       "      <td>in less than quadratic time</td>\n",
       "      <td>Similarly, algorithms can solve the NP-complet...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20808</th>\n",
       "      <td>between 1000 to 1500 BC</td>\n",
       "      <td>Celtic tribes settled in Switzerland between 1...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16151</th>\n",
       "      <td>executive director of football operations</td>\n",
       "      <td>Foster appointed legendary Darrel \"Mouse\" Davi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55832</th>\n",
       "      <td>tobacco, cotton and agriculture</td>\n",
       "      <td>Impoverished by the Civil War, the state conti...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37697</th>\n",
       "      <td>large tumour on her liver</td>\n",
       "      <td>When Lady Flora died in July, the post-mortem ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  answer  \\\n",
       "85209                conduct surveys of party colleagues   \n",
       "20338                   Robert Bideleux and Ian Jeffries   \n",
       "22367                          in excess of £3.3 billion   \n",
       "64526  Koninklijk Conservatorium Artesis Hogeschool A...   \n",
       "97009                                 end of World War I   \n",
       "54390            partly cold-based and partly warm-based   \n",
       "90084                           body and blood of Christ   \n",
       "39942                    the eastern waterfront in Buceo   \n",
       "95598            School of Social Service Administration   \n",
       "58652                    protruded from the road surface   \n",
       "56345        scientific naturalism over natural theology   \n",
       "25781                            Kraftwerk, Art of Noise   \n",
       "38132                    a great-great grandson of Jacob   \n",
       "41885                 off Australia's northwestern coast   \n",
       "64614                       Chris Thile of California is   \n",
       "89455                        in less than quadratic time   \n",
       "20808                            between 1000 to 1500 BC   \n",
       "16151          executive director of football operations   \n",
       "55832                    tobacco, cotton and agriculture   \n",
       "37697                          large tumour on her liver   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "85209  For instance, to keep their party colleagues \"...          5  \n",
       "20338  Significant legislative changes in the status ...          5  \n",
       "22367  The total annual cost to support the defence e...          5  \n",
       "64526  She is now also professor mandolin at the musi...          5  \n",
       "97009  At the end of World War I, the Rhineland was s...          5  \n",
       "54390  Glaciers which are partly cold-based and partl...          5  \n",
       "90084  Luther insisted on the Real Presence of the bo...          5  \n",
       "39942  The Museo Naval, is located on the eastern wat...          5  \n",
       "95598  In 1955, Eero Saarinen was contracted to devel...          5  \n",
       "58652  However, the company ceased trading in 1875 af...          5  \n",
       "56345  Huxley wanted science to be secular, without r...          5  \n",
       "25781  This sound, also influenced by European electr...          5  \n",
       "38132  Jacob and his sons had lived in Canaan but wer...          5  \n",
       "41885  On 20 May 2011, Royal Dutch Shell's final inve...          5  \n",
       "64614  Chris Thile of California is a well-known play...          5  \n",
       "89455  Similarly, algorithms can solve the NP-complet...          5  \n",
       "20808  Celtic tribes settled in Switzerland between 1...          5  \n",
       "16151  Foster appointed legendary Darrel \"Mouse\" Davi...          5  \n",
       "55832  Impoverished by the Civil War, the state conti...          5  \n",
       "37697  When Lady Flora died in July, the post-mortem ...          5  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 5].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the words increase it seems harder to create deceptive incorrect answers. A viable option for some would to be mix the individual words like:\n",
    "\n",
    "*end of World War I\" -> start of World War 1, end of World War II, start of World War II, end of Balkans Wars*....\n",
    "\n",
    "*large tumour on her liver -> large tumor on her brain, large tumor on her lungs, large (some other medical term) on her liver*\n",
    "\n",
    "Though this would become more difficult because if use 2 generated words, they must also fit with each other as well as the original words.\n",
    "\n",
    "Some of the anwers look like logical phrases. For their generation I would argue that a text-summarization aproach would work. And with longer answers we could employ a **True/False** questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>to saturate broken (\"dangling\") bonds of amorp...</td>\n",
       "      <td>Hydrogen is employed to saturate broken (\"dang...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70704</th>\n",
       "      <td>Bullied for being a Bedouin, he was proud of h...</td>\n",
       "      <td>Bullied for being a Bedouin, he was proud of h...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>the Bill &amp; Melinda Gates Foundation Trust, whi...</td>\n",
       "      <td>In October 2006, the Bill &amp; Melinda Gates Foun...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21062</th>\n",
       "      <td>There are 64 possible codons (four possible nu...</td>\n",
       "      <td>:6 Additionally, a \"start codon\", and three \"s...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>Francesinha (Frenchie) from Porto, and bifanas...</td>\n",
       "      <td>Typical fast food dishes include the Francesin...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34718</th>\n",
       "      <td>into four summaries that look specifically at ...</td>\n",
       "      <td>However, results can be further simplified int...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26946</th>\n",
       "      <td>elected members and special office bearers suc...</td>\n",
       "      <td>The legislature consists of elected members an...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96100</th>\n",
       "      <td>support from China for a planned $2.5 billion ...</td>\n",
       "      <td>Kenyatta was \"[a]ccompanied by 60 Kenyan busin...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77789</th>\n",
       "      <td>On 26 December 1999, Chelsea became the first ...</td>\n",
       "      <td>On 26 December 1999, Chelsea became the first ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97887</th>\n",
       "      <td>format of the congress and many specifics of t...</td>\n",
       "      <td>Nevertheless, the format of the congress and m...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  answer  \\\n",
       "14715  to saturate broken (\"dangling\") bonds of amorp...   \n",
       "70704  Bullied for being a Bedouin, he was proud of h...   \n",
       "39422  the Bill & Melinda Gates Foundation Trust, whi...   \n",
       "21062  There are 64 possible codons (four possible nu...   \n",
       "5882   Francesinha (Frenchie) from Porto, and bifanas...   \n",
       "34718  into four summaries that look specifically at ...   \n",
       "26946  elected members and special office bearers suc...   \n",
       "96100  support from China for a planned $2.5 billion ...   \n",
       "77789  On 26 December 1999, Chelsea became the first ...   \n",
       "97887  format of the congress and many specifics of t...   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "14715  Hydrogen is employed to saturate broken (\"dang...         20  \n",
       "70704  Bullied for being a Bedouin, he was proud of h...         20  \n",
       "39422  In October 2006, the Bill & Melinda Gates Foun...         20  \n",
       "21062  :6 Additionally, a \"start codon\", and three \"s...         20  \n",
       "5882   Typical fast food dishes include the Francesin...         20  \n",
       "34718  However, results can be further simplified int...         20  \n",
       "26946  The legislature consists of elected members an...         20  \n",
       "96100  Kenyatta was \"[a]ccompanied by 60 Kenyan busin...         20  \n",
       "77789  On 26 December 1999, Chelsea became the first ...         20  \n",
       "97887  Nevertheless, the format of the congress and m...         20  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 20].sample(n=10, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up,'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 20].sample(n=20, random_state=5).iloc[8]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that from this sentence could be created several questions with single word answers, like:\n",
    "- In what year? - *1999*\n",
    "- Which team? - *Chelsea*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our longest answer with 46 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that the sudden shift of a huge quantity of water into the region could have relaxed the tension between the two sides of the fault, allowing them to move apart, and could have increased the direct pressure on it, causing a violent rupture'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 46].iloc[0]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sudden shift of a huge quantity of water* seems like a good answer to the question *What could have relaxed the tension between the two sides?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hillary Clinton (2008), Howard Dean (2004), Gary Hart (1984 and 1988), Paul Tsongas (1992), Pat Robertson (1988) and Jerry Brown (1976, 1980, 1992).'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 42].iloc[0]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second longest answer seems to be a sequence of correct answer, to something like *Who has been a presitend candidate*. This could be great for queastion with multiple correct answers as well as multiple incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerForWord(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entitiesFound = len(doc.ents)\n",
    "    \n",
    "    if (entitiesFound > 0):\n",
    "        #TODO - Could potentially find multiple entities in the text. We're returning only the first one.\n",
    "        return doc.ents[0].label_\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NerForWord('Portugal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " University_of_Notre_Dame\n",
      " Beyoncé\n",
      "GPE Montana\n",
      " Genocide\n",
      " Antibiotics\n",
      " Frédéric_Chopin\n",
      " Sino-Tibetan_relations_during_the_Ming_dynasty\n",
      "ORG IPod\n",
      " The_Legend_of_Zelda:_Twilight_Princess\n",
      " Spectre_(2015_film)\n",
      "CARDINAL 2008_Sichuan_earthquake\n",
      " New_York_City\n",
      " To_Kill_a_Mockingbird\n",
      " Solar_energy\n",
      "NORP Tajikistan\n",
      " Anthropology\n",
      "GPE Portugal\n",
      " Kanye_West\n",
      " Buddhism\n",
      " American_Idol\n",
      " Dog\n",
      " 2008_Summer_Olympics_torch_relay\n",
      " Alfred_North_Whitehead\n",
      " Financial_crisis_of_2007%E2%80%9308\n",
      " Saint_Barth%C3%A9lemy\n",
      " Genome\n",
      " Comprehensive_school\n",
      " Republic_of_the_Congo\n",
      " Prime_minister\n",
      " Institute_of_technology\n",
      " Wayback_Machine\n",
      " Dutch_Republic\n",
      "ORG Symbiosis\n",
      " Canadian_Armed_Forces\n",
      " Cardinal_(Catholicism)\n",
      " Iranian_languages\n",
      " Lighting\n",
      " Separation_of_powers_under_the_United_States_Constitution\n",
      " Architecture\n",
      " Human_Development_Index\n",
      " Southern_Europe\n",
      " BBC_Television\n",
      " Arnold_Schwarzenegger\n",
      "GPE Plymouth\n",
      "GPE Heresy\n",
      " Warsaw_Pact\n",
      " Materialism\n",
      " Space_Race\n",
      " Pub\n",
      "NORP Christian\n",
      " Sony_Music_Entertainment\n",
      " Oklahoma_City\n",
      "PERSON Hunter-gatherer\n",
      " United_Nations_Population_Fund\n",
      " Russian_Soviet_Federative_Socialist_Republic\n",
      " Universal_Studios\n",
      " Alexander_Graham_Bell\n",
      " Internet_service_provider\n",
      " Comics\n",
      " Saint_Helena\n",
      " Aspirated_consonant\n",
      "ORG Hydrogen\n",
      " Web_browser\n",
      "GPE Boston\n",
      " BeiDou_Navigation_Satellite_System\n",
      " Canon_law\n",
      "GPE Communications_in_Somalia\n",
      " Catalan_language\n",
      " Estonian_language\n",
      " Paper\n",
      " Arena_Football_League\n",
      " Adult_contemporary_music\n",
      " Matter\n",
      " Westminster_Abbey\n",
      "GPE Nanjing\n",
      " Bern\n",
      " Daylight_saving_time\n",
      " Royal_Institute_of_British_Architects\n",
      " National_Archives_and_Records_Administration\n",
      " Tristan_da_Cunha\n",
      " University_of_Kansas\n",
      " Political_corruption\n",
      " Dialect\n",
      " Classical_music\n",
      "GPE Slavs\n",
      "ORG Southampton\n",
      " Treaty\n",
      " Josip_Broz_Tito\n",
      " Marshall_Islands\n",
      " Szlachta\n",
      "PERSON Virgil\n",
      " Alps\n",
      " Gene\n",
      "GPE Guinea-Bissau\n",
      "GPE List_of_numbered_streets_in_Manhattan\n",
      " Brain\n",
      " Near_East\n",
      " Zhejiang\n",
      " Ministry_of_Defence_(United_Kingdom)\n",
      " High-definition_television\n",
      " Wood\n",
      "NORP Somalis\n",
      " Middle_Ages\n",
      " Phonology\n",
      " Computer\n",
      " Black_people\n",
      " The_Times\n",
      " New_Delhi\n",
      " Imamah_(Shia_doctrine)\n",
      " Bird_migration\n",
      " Atlantic_City,_New_Jersey\n",
      " Immunology\n",
      " MP3\n",
      " House_music\n",
      " Letter_case\n",
      "ORG Chihuahua_(state)\n",
      " Pitch_(music)\n",
      " England_national_football_team\n",
      "GPE Houston\n",
      " Copper\n",
      " Identity_(social_science)\n",
      " Himachal_Pradesh\n",
      " Communication\n",
      " Grape\n",
      " Computer_security\n",
      " Orthodox_Judaism\n",
      " Animal\n",
      " Beer\n",
      " Race_and_ethnicity_in_the_United_States_Census\n",
      " United_States_dollar\n",
      " Imperial_College_London\n",
      " Gymnastics\n",
      " Hanover\n",
      " Emotion\n",
      " FC_Barcelona\n",
      " Everton_F.C.\n",
      " Old_English\n",
      " Aircraft_carrier\n",
      " Federal_Aviation_Administration\n",
      " Lancashire\n",
      " Mesozoic\n",
      " Videoconferencing\n",
      " Gregorian_calendar\n",
      " Xbox_360\n",
      " Military_history_of_the_United_States\n",
      " Hard_rock\n",
      " Great_Plains\n",
      " Infrared\n",
      " Biodiversity\n",
      "GPE ASCII\n",
      " Digestion\n",
      " Federal_Bureau_of_Investigation\n",
      " Adolescence\n",
      " Antarctica\n",
      " Mary_(mother_of_Jesus)\n",
      " Melbourne\n",
      "GPE John,_King_of_England\n",
      "ORG Macintosh\n",
      " Anti-aircraft_warfare\n",
      " Sanskrit\n",
      " Valencia\n",
      " General_Electric\n",
      " United_States_Army\n",
      "PERSON Franco-Prussian_War\n",
      "GPE Eritrea\n",
      " Uranium\n",
      " Order_of_the_British_Empire\n",
      " Age_of_Enlightenment\n",
      " Circadian_rhythm\n",
      " Elizabeth_II\n",
      " Sexual_orientation\n",
      " Dell\n",
      " Capital_punishment_in_the_United_States\n",
      " Nintendo_Entertainment_System\n",
      " Ashkenazi_Jews\n",
      "GPE Athanasius_of_Alexandria\n",
      "GPE Seattle\n",
      " Memory\n",
      " Multiracial_American\n",
      " Pharmaceutical_industry\n",
      " Umayyad_Caliphate\n",
      " Asphalt\n",
      " Queen_Victoria\n",
      " Freemasonry\n",
      "GPE Israel\n",
      " Hellenistic_period\n",
      " Napoleon\n",
      " Bill_%26_Melinda_Gates_Foundation\n",
      " Northwestern_University\n",
      " Hokkien\n",
      " Montevideo\n",
      " Poultry\n",
      " Arsenal_F.C.\n",
      " Dutch_language\n",
      " Buckingham_Palace\n",
      " Incandescent_light_bulb\n",
      " Clothing\n",
      " Chicago_Cubs\n",
      " States_of_Germany\n",
      "GPE Korean_War\n",
      " Royal_Dutch_Shell\n",
      " Copyright_infringement\n",
      "GPE Greece\n",
      " Mammal\n",
      " East_India_Company\n",
      " Southeast_Asia\n",
      " Professional_wrestling\n",
      " Film_speed\n",
      " Mexico_City\n",
      "NORP Germans\n",
      " New_Haven,_Connecticut\n",
      " Brigham_Young_University\n",
      " Myocardial_infarction\n",
      " Department_store\n",
      " Intellectual_property\n",
      "GPE Florida\n",
      "GPE Queen_(band)\n",
      " Presbyterianism\n",
      " Thuringia\n",
      " Predation\n",
      " Marvel_Comics\n",
      " British_Empire\n",
      "GPE Botany\n",
      "ORG Madonna_(entertainer)\n",
      "GPE London\n",
      " Law_of_the_United_States\n",
      "GPE Myanmar\n",
      "NORP Jews\n",
      " Cotton\n",
      " Data_compression\n",
      " The_Sun_(United_Kingdom)\n",
      " Carnival\n",
      " Pesticide\n",
      " Somerset\n",
      " Yale_University\n",
      " Late_Middle_Ages\n",
      " Ann_Arbor,_Michigan\n",
      " Gothic_architecture\n",
      " Cubism\n",
      " Political_philosophy\n",
      " Alloy\n",
      " Norfolk_Island\n",
      " Edmund_Burke\n",
      "GPE Samoa\n",
      " Pope_Paul_VI\n",
      " George_VI\n",
      " Electric_motor\n",
      "GPE Switzerland\n",
      " Mali\n",
      " Nonprofit_organization\n",
      "GPE Raleigh,_North_Carolina\n",
      " Nutrition\n",
      " Crimean_War\n",
      " Literature\n",
      " Avicenna\n",
      " Chinese_characters\n",
      "PERSON Bermuda\n",
      " Nigeria\n",
      "GPE Utrecht\n",
      " John_von_Neumann\n",
      " Molotov%E2%80%93Ribbentrop_Pact\n",
      " Capacitor\n",
      " History_of_science\n",
      " Czech_language\n",
      " Digimon\n",
      " Glacier\n",
      " Planck_constant\n",
      " Comcast\n",
      " Tuberculosis\n",
      " Affirmative_action_in_the_United_States\n",
      " FA_Cup\n",
      " Alsace\n",
      " Baptists\n",
      " Child_labour\n",
      " North_Carolina\n",
      " Heian_period\n",
      " On_the_Origin_of_Species\n",
      " Dissolution_of_the_Soviet_Union\n",
      " Crucifixion_of_Jesus\n",
      "GPE Miami\n",
      " Supreme_court\n",
      " Textual_criticism\n",
      " Gramophone_record\n",
      " Turner_Classic_Movies\n",
      " Hindu_philosophy\n",
      " Political_party\n",
      " A_cappella\n",
      " Dominican_Order\n",
      " Eton_College\n",
      " Cork_(city)\n",
      " Federalism\n",
      " Galicia_(Spain)\n",
      " Green\n",
      " USB\n",
      "GPE Sichuan\n",
      " Unicode\n",
      "GPE Detroit\n",
      " Culture\n",
      " Sahara\n",
      " Rule_of_law\n",
      "GPE Tibet\n",
      " Exhibition_game\n",
      "GPE Strasbourg\n",
      "GPE Oklahoma\n",
      " History_of_India\n",
      " Gamal_Abdel_Nasser\n",
      " Pope_John_XXIII\n",
      " Time\n",
      "ORG European_Central_Bank\n",
      " St._John%27s,_Newfoundland_and_Labrador\n",
      " PlayStation_3\n",
      " Royal_assent\n",
      " Group_(mathematics)\n",
      "ORG Central_African_Republic\n",
      " Asthma\n",
      "ORG LaserDisc\n",
      "ORG Annelid\n",
      " God\n",
      " War_on_Terror\n",
      " Labour_Party_(UK)\n",
      "GPE Estonia\n",
      " Serbo-Croatian\n",
      "GPE Alaska\n",
      " Karl_Popper\n",
      "ORG Mandolin\n",
      " Insect\n",
      " Race_(human_categorization)\n",
      "GPE Paris\n",
      " Apollo\n",
      " United_States_presidential_election,_2004\n",
      "ORG IBM\n",
      " Liberal_Party_of_Australia\n",
      " Samurai\n",
      " Software_testing\n",
      "PERSON Glass\n",
      " Renewable_energy_commercialization\n",
      "GPE Palermo\n",
      " Zinc\n",
      " Neoclassical_architecture\n",
      " CBC_Television\n",
      " Appalachian_Mountains\n",
      " Energy\n",
      "GPE East_Prussia\n",
      " Ottoman_Empire\n",
      " Philosophy_of_space_and_time\n",
      " Neolithic\n",
      " Friedrich_Hayek\n",
      "PERSON Diarrhea\n",
      " Madrasa\n",
      "GPE Philadelphia\n",
      " John_Kerry\n",
      " Rajasthan\n",
      "GPE Guam\n",
      " Empiricism\n",
      " Idealism\n",
      " Education\n",
      "GPE Tennessee\n",
      " Post-punk\n",
      " Canadian_football\n",
      " Seven_Years%27_War\n",
      " Richard_Feynman\n",
      " Muammar_Gaddafi\n",
      " Cyprus\n",
      " Steven_Spielberg\n",
      " Elevator\n",
      " Neptune\n",
      " Railway_electrification_system\n",
      " Spanish_language_in_the_United_States\n",
      " Charleston,_South_Carolina\n",
      " Red\n",
      " The_Blitz\n",
      " Endangered_Species_Act\n",
      " Vacuum\n",
      " Han_dynasty\n",
      " Greeks\n",
      " Quran\n",
      " Great_power\n",
      " Geography_of_the_United_States\n",
      " Compact_disc\n",
      " Transistor\n",
      " Modern_history\n",
      " 51st_state\n",
      "PERSON Antenna_(radio)\n",
      " Flowering_plant\n",
      "GPE Hyderabad\n",
      "GPE Santa_Monica,_California\n",
      " Washington_University_in_St._Louis\n",
      " Central_Intelligence_Agency\n",
      " Pain\n",
      " Database\n",
      " Tucson,_Arizona\n",
      "GPE Armenia\n",
      "GPE Bacteria\n",
      " Printed_circuit_board\n",
      " Premier_League\n",
      " Roman_Republic\n",
      "ORG Pacific_War\n",
      "GPE Richmond,_Virginia\n",
      "ORG San_Diego\n",
      " Muslim_world\n",
      "GPE Iran\n",
      " British_Isles\n",
      " Association_football\n",
      " Georgian_architecture\n",
      " Liberia\n",
      " Windows_8\n",
      " Swaziland\n",
      " Translation\n",
      " Airport\n",
      " Kievan_Rus%27\n",
      " Super_Nintendo_Entertainment_System\n",
      " Sumer\n",
      "ORG Tuvalu\n",
      " Immaculate_Conception\n",
      "GPE Namibia\n",
      " Russian_language\n",
      " United_States_Air_Force\n",
      " Light-emitting_diode\n",
      "GPE Bird\n",
      " Qing_dynasty\n",
      " Indigenous_peoples_of_the_Americas\n",
      "GPE Egypt\n",
      " Mosaic\n",
      " University\n",
      " Religion_in_ancient_Rome\n",
      "ORG YouTube\n",
      " Separation_of_church_and_state_in_the_United_States\n",
      " Protestantism\n",
      "GPE Bras%C3%ADlia\n",
      " Economy_of_Greece\n",
      " Party_leaders_of_the_United_States_House_of_Representatives\n",
      "NORP Armenians\n",
      " Jehovah%27s_Witnesses\n",
      " Dwight_D._Eisenhower\n",
      " The_Bronx\n",
      " Humanism\n",
      " Geological_history_of_Earth\n",
      " Police\n",
      "GPE Punjab,_Pakistan\n",
      " Infection\n",
      " Hunting\n",
      " Kathmandu\n",
      " Super_Bowl_50\n",
      "GPE Warsaw\n",
      "ORG Normans\n",
      " Nikola_Tesla\n",
      " Computational_complexity_theory\n",
      " Teacher\n",
      " Martin_Luther\n",
      " Southern_California\n",
      " Sky_(United_Kingdom)\n",
      "GPE Victoria_(Australia)\n",
      " Huguenot\n",
      " Steam_engine\n",
      "ORG Oxygen\n",
      "CARDINAL 1973_oil_crisis\n",
      " Apollo_program\n",
      " European_Union_law\n",
      " Amazon_rainforest\n",
      "PERSON Ctenophora\n",
      " Fresno,_California\n",
      " Packet_switching\n",
      " Black_Death\n",
      " Geology\n",
      " Newcastle_upon_Tyne\n",
      " Victoria_and_Albert_Museum\n",
      " American_Broadcasting_Company\n",
      " Genghis_Khan\n",
      " Pharmacy\n",
      " Immune_system\n",
      " Civil_disobedience\n",
      " Construction\n",
      " Private_school\n",
      " Harvard_University\n",
      " Jacksonville,_Florida\n",
      " Economic_inequality\n",
      " Doctor_Who\n",
      "CARDINAL University_of_Chicago\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yuan_dynasty\n",
      "GPE Kenya\n",
      " Intergovernmental_Panel_on_Climate_Change\n",
      " Chloroplast\n",
      " Prime_number\n",
      " Rhine\n",
      " Scottish_Parliament\n",
      " Islamism\n",
      " Imperialism\n",
      " United_Methodist_Church\n",
      " French_and_Indian_War\n",
      " Force\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(NerForWord(title), title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
